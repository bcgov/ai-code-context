# Repository Snapshot (LLM-Optimized)

Generated On: 2025-11-05T21:21:25.433Z

# Mnemonic Weight (Token Count): ~7,818 tokens

# Directory Structure (relative to project root)
  ./.env.example
  ./README.md
  ./auth.js
  ./config.json
  ./config.yaml
  ./data_analysis.ipynb
  ./data_processor.py
  ./deploy.sh
  ./pyproject.toml
  ./schema.sql

--- START OF FILE ./.env.example ---

# Sample Environment Variables
# Copy this file to .env and fill in your actual values
# This file demonstrates environment variable handling in snapshots

# Application
NODE_ENV=development
PORT=3000
HOST=localhost
DEBUG=true

# Database
DATABASE_URL=postgresql://username:password@localhost:5432/sample_db
DB_SSL=false
DB_POOL_MIN=2
DB_POOL_MAX=20

# Authentication
JWT_SECRET=your-super-secret-jwt-key-change-in-production
JWT_EXPIRES_IN=24h
BCRYPT_ROUNDS=12

# Email Service
EMAIL_PROVIDER=sendgrid
SENDGRID_API_KEY=your-sendgrid-api-key
EMAIL_FROM_NAME=Sample Project
EMAIL_FROM_EMAIL=noreply@example.com

# OAuth Providers
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret

# File Upload
UPLOAD_MAX_SIZE=10485760
UPLOAD_DESTINATION=./uploads
UPLOAD_ALLOWED_TYPES=image/jpeg,image/png,image/gif,application/pdf

# External Services
GOOGLE_ANALYTICS_ID=GA-XXXXXXXXX
MIXPANEL_TOKEN=your-mixpanel-token
SENTRY_DSN=your-sentry-dsn
DATADOG_API_KEY=your-datadog-api-key

# AWS S3 (if using S3 for file storage)
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
AWS_S3_BUCKET=sample-project-uploads
AWS_REGION=us-east-1

# Redis (if using Redis for caching/sessions)
REDIS_URL=redis://localhost:6379
REDIS_PASSWORD=
REDIS_DB=0

# API Keys (these should be kept secret!)
OPENAI_API_KEY=[REDACTED_LONG_KEY]
STRIPE_SECRET_KEY=sk_test_your-stripe-secret-key
STRIPE_PUBLISHABLE_KEY=pk_test_your-stripe-publishable-key

# Feature Flags
ENABLE_USER_REGISTRATION=true
ENABLE_EMAIL_VERIFICATION=true
ENABLE_TWO_FACTOR_AUTH=false
ENABLE_ANALYTICS=true

# Logging
LOG_LEVEL=info
LOG_FILE=./logs/app.log
LOG_MAX_SIZE=10m
LOG_MAX_FILES=5

# Security
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,https://app.example.com
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# Development
HOT_RELOAD=true
SOURCE_MAPS=true
ERROR_DETAILS=true

[NOTE] Sensitive values were detected and redacted in this file.
--- END OF FILE ---

--- START OF FILE ./README.md ---

# Sample Project

This is a sample project to demonstrate the repository snapshot functionality.

## Features

- User authentication
- Data processing pipeline
- API endpoints
- Documentation

## Installation

```bash
npm install
pip install -r requirements.txt
```

## Usage

```javascript
import { authenticateUser } from './auth.js';

const user = await authenticateUser('username', 'password');
```

## API Documentation

See `api.md` for detailed API documentation.

## Contributing

Please read `CONTRIBUTING.md` for contribution guidelines.
--- END OF FILE ---

--- START OF FILE ./auth.js ---

/**
 * Authentication utilities for the sample project
 * @module auth
 */

import bcrypt from 'bcrypt';
import jwt from 'jsonwebtoken';
import { readFileSync } from 'fs';
import path from 'path';

/**
 * User authentication class
 */
export class AuthService {
    constructor(configPath = './config.json') {
        this.config = this.loadConfig(configPath);
        this.users = new Map(); // In production, this would be a database
    }

    /**
     * Load configuration from JSON file
     * @param {string} configPath - Path to config file
     * @returns {Object} Configuration object
     */
    loadConfig(configPath) {
        try {
            const configData = readFileSync(configPath, 'utf8');
            return JSON.parse(configData);
        } catch (error) {
            console.warn('Config file not found, using defaults');
            return {
                jwtSecret: 'default-secret-key',
                bcryptRounds: 10,
                sessionTimeout: 3600
            };
        }
    }

    /**
     * Hash a password using bcrypt
     * @param {string} password - Plain text password
     * @returns {Promise<string>} Hashed password
     */
    async hashPassword(password) {
        const saltRounds = this.config.bcryptRounds || 10;
        return await bcrypt.hash(password, saltRounds);
    }

    /**
     * Verify a password against its hash
     * @param {string} password - Plain text password
     * @param {string} hash - Hashed password
     * @returns {Promise<boolean>} True if password matches
     */
    async verifyPassword(password, hash) {
        return await bcrypt.compare(password, hash);
    }

    /**
     * Authenticate a user
     * @param {string} username - Username
     * @param {string} password - Password
     * @returns {Promise<Object|null>} User object with token or null
     */
    async authenticateUser(username, password) {
        const user = this.users.get(username);

        if (!user) {
            return null;
        }

        const isValidPassword = await this.verifyPassword(password, user.passwordHash);

        if (!isValidPassword) {
            return null;
        }

        // Generate JWT token
        const token = jwt.sign(
            {
                userId: user.id,
                username: user.username,
                role: user.role
            },
            this.config.jwtSecret,
            { expiresIn: this.config.sessionTimeout }
        );

        return {
            id: user.id,
            username: user.username,
            role: user.role,
            token,
            expiresAt: Date.now() + (this.config.sessionTimeout * 1000)
        };
    }

    /**
     * Register a new user
     * @param {string} username - Username
     * @param {string} password - Password
     * @param {string} email - Email address
     * @param {string} role - User role (default: 'user')
     * @returns {Promise<Object>} Created user object
     */
    async registerUser(username, password, email, role = 'user') {
        if (this.users.has(username)) {
            throw new Error('Username already exists');
        }

        const passwordHash = await this.hashPassword(password);
        const userId = Date.now().toString(); // Simple ID generation

        const user = {
            id: userId,
            username,
            email,
            passwordHash,
            role,
            createdAt: new Date().toISOString()
        };

        this.users.set(username, user);
        return { id: user.id, username: user.username, email: user.email, role: user.role };
    }

    /**
     * Verify JWT token
     * @param {string} token - JWT token
     * @returns {Object|null} Decoded token payload or null
     */
    verifyToken(token) {
        try {
            return jwt.verify(token, this.config.jwtSecret);
        } catch (error) {
            return null;
        }
    }

    /**
     * Middleware function for Express.js authentication
     * @param {Object} req - Express request object
     * @param {Object} res - Express response object
     * @param {Function} next - Express next function
     */
    authenticateMiddleware(req, res, next) {
        const authHeader = req.headers.authorization;

        if (!authHeader || !authHeader.startsWith('Bearer ')) {
            return res.status(401).json({ error: 'No token provided' });
        }

        const token = authHeader.substring(7);
        const decoded = this.verifyToken(token);

        if (!decoded) {
            return res.status(401).json({ error: 'Invalid token' });
        }

        req.user = decoded;
        next();
    }
}

/**
 * Create default auth service instance
 */
export const authService = new AuthService();

/**
 * Convenience functions for common operations
 */
export const authenticateUser = (username, password) =>
    authService.authenticateUser(username, password);

export const registerUser = (username, password, email, role) =>
    authService.registerUser(username, password, email, role);

export const verifyToken = (token) =>
    authService.verifyToken(token);

export default AuthService;
--- END OF FILE ---

--- START OF FILE ./config.json ---

{
  "app": {
    "name": "Sample Project",
    "version": "1.0.0",
    "environment": "development",
    "port": 3000,
    "host": "localhost"
  },
  "database": {
    "type": "postgresql",
    "host": "localhost",
    "port": 5432,
    "database": "sample_db",
    "username": "sample_user",
    "password": "sample_password",
    "pool": {
      "min": 2,
      "max": 10,
      "idle": 10000
    }
  },
  "auth": {
    "jwtSecret": "your-super-secret-jwt-key-change-in-production",
    "bcryptRounds": 12,
    "sessionTimeout": 3600,
    "refreshTokenExpiry": 604800
  },
  "api": {
    "rateLimit": {
      "windowMs": 900000,
      "max": 100
    },
    "cors": {
      "origin": ["http://localhost:3000", "http://localhost:3001"],
      "credentials": true
    }
  },
  "logging": {
    "level": "info",
    "file": "logs/app.log",
    "maxSize": "10m",
    "maxFiles": "5"
  },
  "features": {
    "userRegistration": true,
    "emailVerification": true,
    "passwordReset": true,
    "twoFactorAuth": false
  }
}
--- END OF FILE ---

--- START OF FILE ./config.yaml ---

# Sample YAML Configuration
# This demonstrates YAML format support in snapshots

# Application settings
app:
  name: "Sample Project"
  version: "1.0.0"
  environment: "development"
  debug: true
  port: 3000
  host: "0.0.0.0"

# Database configuration
database:
  type: "postgresql"
  host: "localhost"
  port: 5432
  name: "sample_db"
  username: "sample_user"
  password: "sample_password"
  ssl: false
  connection_pool:
    min: 2
    max: 20
    idle_timeout: 30000
    acquire_timeout: 60000

# Authentication settings
auth:
  jwt:
    secret: "your-super-secret-jwt-key-change-in-production"
    algorithm: "HS256"
    expires_in: "24h"
    refresh_expires_in: "7d"
  bcrypt:
    rounds: 12
  oauth:
    google:
      client_id: "your-google-client-id"
      client_secret: "your-google-client-secret"
    github:
      client_id: "your-github-client-id"
      client_secret: "your-github-client-secret"

# API configuration
api:
  prefix: "/api/v1"
  rate_limiting:
    enabled: true
    window_ms: 900000
    max_requests: 100
  cors:
    enabled: true
    origins:
      - "http://localhost:3000"
      - "http://localhost:3001"
      - "https://app.example.com"
    credentials: true
    methods:
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
      - "OPTIONS"
    headers:
      - "Content-Type"
      - "Authorization"
      - "X-Requested-With"

# Logging configuration
logging:
  level: "info"
  format: "json"
  outputs:
    - type: "console"
      colorize: true
    - type: "file"
      path: "./logs/app.log"
      max_size: "10m"
      max_files: "5"
  categories:
    http:
      level: "info"
    database:
      level: "warn"
    auth:
      level: "debug"

# Email configuration
email:
  provider: "sendgrid"
  api_key: "your-sendgrid-api-key"
  from:
    name: "Sample Project"
    email: "noreply@example.com"
  templates:
    welcome: "templates/welcome.html"
    password_reset: "templates/password-reset.html"
    email_verification: "templates/email-verification.html"

# File upload settings
uploads:
  max_file_size: "10MB"
  allowed_types:
    - "image/jpeg"
    - "image/png"
    - "image/gif"
    - "application/pdf"
  destination: "./uploads"
  temp_dir: "./tmp/uploads"

# Feature flags
features:
  user_registration: true
  email_verification: true
  password_reset: true
  two_factor_auth: false
  analytics: true
  notifications: true
  api_documentation: true

# External service integrations
integrations:
  analytics:
    google_analytics_id: "GA-XXXXXXXXX"
    mixpanel_token: "your-mixpanel-token"
  monitoring:
    sentry_dsn: "your-sentry-dsn"
    datadog_api_key: "your-datadog-api-key"
  storage:
    aws_s3:
      bucket: "sample-project-uploads"
      region: "us-east-1"
      access_key_id: "your-aws-access-key"
      secret_access_key: "your-aws-secret-key"

# Development settings
development:
  hot_reload: true
  source_maps: true
  error_details: true
  cors_any_origin: true

# Production settings
production:
  optimize_assets: true
  gzip_compression: true
  security_headers: true
  rate_limiting_strict: true
--- END OF FILE ---

--- START OF FILE ./data_analysis.ipynb ---

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8aabfd99",
      "metadata": {},
      "source": [
        "# Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates data analysis capabilities for the sample project.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- Load and process user data\n",
        "- Generate visualizations\n",
        "- Create summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e384942c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c11aec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample user data\n",
        "user_data = [\n",
        "    {'name': 'Alice Johnson', 'email': 'alice@example.com', 'age': 28, 'signup_date': '2023-01-15', 'plan': 'premium'},\n",
        "    {'name': 'Bob Smith', 'email': 'bob@example.com', 'age': 34, 'signup_date': '2023-02-20', 'plan': 'basic'},\n",
        "    {'name': 'Charlie Brown', 'email': 'charlie@example.com', 'age': 22, 'signup_date': '2023-03-10', 'plan': 'premium'},\n",
        "    {'name': 'Diana Prince', 'email': 'diana@example.com', 'age': 31, 'signup_date': '2023-04-05', 'plan': 'basic'},\n",
        "    {'name': 'Eve Wilson', 'email': 'eve@example.com', 'age': 26, 'signup_date': '2023-05-12', 'plan': 'premium'}\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(user_data)\n",
        "df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
        "\n",
        "print(\"Sample data loaded:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a12473",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Basic Statistics:\")\n",
        "print(f\"Total users: {len(df)}\")\n",
        "print(f\"Average age: {df['age'].mean():.1f} years\")\n",
        "print(f\"Age range: {df['age'].min()} - {df['age'].max()} years\")\n",
        "print(f\"\\nPlan distribution:\")\n",
        "print(df['plan'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950a2014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create age distribution plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['age'], bins=5, edgecolor='black', alpha=0.7)\n",
        "plt.title('User Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc391b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly signup trend\n",
        "monthly_signups = df.groupby(df['signup_date'].dt.to_period('M')).size()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_signups.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title('Monthly User Signups')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Signups')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36f3387",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "- Data loading and preprocessing\n",
        "- Basic statistical analysis\n",
        "- Data visualization with matplotlib\n",
        "- Time series analysis of user signups\n",
        "\n",
        "### Key Findings:\n",
        "- Average user age: 28.2 years\n",
        "- Most popular plan: Premium (60% of users)\n",
        "- Steady growth in user signups over time"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
--- END OF FILE ---

--- START OF FILE ./data_processor.py ---

#!/usr/bin/env python3
"""
Data processing utilities for the sample project.

This module provides functions for processing user data,
validating inputs, and generating reports.
"""

import json
import datetime
from typing import List, Dict, Optional


class DataProcessor:
    """Main class for processing user data."""

    def __init__(self, config_file: str = 'config.json'):
        self.config = self.load_config(config_file)
        self.processed_count = 0

    def load_config(self, config_file: str) -> Dict:
        """Load configuration from JSON file."""
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {'default_setting': 'value'}

    def validate_user_data(self, user_data: Dict) -> bool:
        """Validate user data structure."""
        required_fields = ['name', 'email', 'age']

        for field in required_fields:
            if field not in user_data:
                return False

        if not isinstance(user_data['age'], int) or user_data['age'] < 0:
            return False

        return True

    def process_users(self, users: List[Dict]) -> List[Dict]:
        """Process a list of user data."""
        processed_users = []

        for user in users:
            if self.validate_user_data(user):
                processed_user = {
                    **user,
                    'processed_at': datetime.datetime.now().isoformat(),
                    'status': 'active'
                }
                processed_users.append(processed_user)
                self.processed_count += 1

        return processed_users

    def generate_report(self, processed_users: List[Dict]) -> str:
        """Generate a summary report."""
        total_users = len(processed_users)
        avg_age = sum(user['age'] for user in processed_users) / total_users if total_users > 0 else 0

        report = f"""
Data Processing Report
======================

Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Summary:
- Total users processed: {total_users}
- Average age: {avg_age:.1f}
- Processing batch size: {self.config.get('batch_size', 'N/A')}

Details:
"""

        for user in processed_users[:5]:  # Show first 5 users
            report += f"- {user['name']} ({user['email']}) - Age: {user['age']}\n"

        if total_users > 5:
            report += f"- ... and {total_users - 5} more users\n"

        return report


def main():
    """Main function for command-line usage."""
    processor = DataProcessor()

    # Sample data
    sample_users = [
        {'name': 'Alice Johnson', 'email': 'alice@example.com', 'age': 28},
        {'name': 'Bob Smith', 'email': 'bob@example.com', 'age': 34},
        {'name': 'Charlie Brown', 'email': 'charlie@example.com', 'age': 22}
    ]

    processed = processor.process_users(sample_users)
    report = processor.generate_report(processed)

    print(report)


if __name__ == '__main__':
    main()
--- END OF FILE ---

--- START OF FILE ./deploy.sh ---

#!/bin/bash

# Sample deployment script for the sample project
# This script demonstrates shell script inclusion in snapshots

set -e  # Exit on any error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Configuration
APP_NAME="sample-project"
DEPLOY_ENV="${1:-development}"
BACKUP_DIR="./backups"
LOG_FILE="./deploy.log"

# Logging function
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $*" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    echo -e "${RED}Error: $1${NC}" >&2
    log "ERROR: $1"
    exit 1
}

# Success message
success() {
    echo -e "${GREEN}âœ“ $1${NC}"
    log "SUCCESS: $1"
}

# Warning message
warning() {
    echo -e "${YELLOW}âš  $1${NC}"
    log "WARNING: $1"
}

# Main deployment function
deploy() {
    log "Starting deployment of $APP_NAME to $DEPLOY_ENV environment"

    # Pre-deployment checks
    check_dependencies
    create_backup
    validate_config

    # Deployment steps
    install_dependencies
    run_migrations
    build_assets
    restart_services

    # Post-deployment
    run_tests
    cleanup

    success "Deployment completed successfully!"
}

# Check system dependencies
check_dependencies() {
    log "Checking system dependencies..."

    command -v node >/dev/null 2>&1 || error_exit "Node.js is required but not installed"
    command -v python3 >/dev/null 2>&1 || error_exit "Python 3 is required but not installed"
    command -v pip >/dev/null 2>&1 || error_exit "pip is required but not installed"

    success "All dependencies are available"
}

# Create backup
create_backup() {
    log "Creating backup..."

    mkdir -p "$BACKUP_DIR"
    BACKUP_FILE="$BACKUP_DIR/backup-$(date +%Y%m%d-%H%M%S).tar.gz"

    if [ -d "data" ]; then
        tar -czf "$BACKUP_FILE" data/ 2>/dev/null || warning "No data directory to backup"
    else
        warning "No data directory found, skipping backup"
    fi

    success "Backup created: $BACKUP_FILE"
}

# Validate configuration
validate_config() {
    log "Validating configuration..."

    if [ ! -f "config.json" ]; then
        error_exit "config.json not found"
    fi

    # Basic JSON validation
    python3 -m json.tool config.json >/dev/null 2>&1 || error_exit "config.json is not valid JSON"

    success "Configuration validated"
}

# Install dependencies
install_dependencies() {
    log "Installing dependencies..."

    if [ -f "package.json" ]; then
        npm install || error_exit "Failed to install Node.js dependencies"
    fi

    if [ -f "requirements.txt" ]; then
        pip install -r requirements.txt || error_exit "Failed to install Python dependencies"
    fi

    success "Dependencies installed"
}

# Run database migrations
run_migrations() {
    log "Running database migrations..."

    # This would typically run actual migration commands
    # For demo purposes, we'll just simulate it
    if [ "$DEPLOY_ENV" = "production" ]; then
        warning "Production deployment detected - running full migration suite"
        # In real scenario: python manage.py migrate
        sleep 1
    else
        warning "Development deployment - skipping heavy migrations"
    fi

    success "Migrations completed"
}

# Build assets
build_assets() {
    log "Building assets..."

    if [ -f "package.json" ] && grep -q '"build"' package.json; then
        npm run build || error_exit "Failed to build assets"
    fi

    success "Assets built"
}

# Restart services
restart_services() {
    log "Restarting services..."

    # In a real scenario, this might restart systemd services, Docker containers, etc.
    # For demo, we'll just simulate
    if pgrep -f "node.*app.js" >/dev/null; then
        warning "Found running Node.js process, restarting..."
        # pkill -f "node.*app.js"
        # npm start &
    fi

    success "Services restarted"
}

# Run tests
run_tests() {
    log "Running tests..."

    if [ -f "package.json" ] && grep -q '"test"' package.json; then
        npm test || warning "Some tests failed"
    fi

    if [ -d "tests" ] && [ -f "requirements.txt" ]; then
        python3 -m pytest tests/ -v || warning "Some Python tests failed"
    fi

    success "Tests completed"
}

# Cleanup
cleanup() {
    log "Cleaning up..."

    # Remove temporary files, clear caches, etc.
    find . -name "*.tmp" -type f -delete 2>/dev/null || true
    find . -name ".DS_Store" -type f -delete 2>/dev/null || true

    success "Cleanup completed"
}

# Health check
health_check() {
    log "Performing health check..."

    # Check if services are responding
    # curl -f http://localhost:3000/health >/dev/null 2>&1 || warning "Health check failed"

    success "Health check passed"
}

# Main execution
main() {
    echo "ðŸš€ $APP_NAME Deployment Script"
    echo "Environment: $DEPLOY_ENV"
    echo "Log file: $LOG_FILE"
    echo

    deploy
    health_check

    echo
    echo -e "${GREEN}ðŸŽ‰ Deployment completed successfully!${NC}"
    echo "Check $LOG_FILE for detailed logs"
}

# Run main function with all arguments
main "$@"
--- END OF FILE ---

--- START OF FILE ./pyproject.toml ---

# Sample TOML Configuration
# This demonstrates TOML format support

[project]
name = "Sample Project"
version = "1.0.0"
description = "A comprehensive sample project for testing"
authors = ["Sample Developer <dev@example.com>"]
license = "MIT"
readme = "README.md"
homepage = "https://example.com"
repository = "https://github.com/example/sample-project"

[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta"

[tool.black]
line-length = 88
target-version = ['py38']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q"
testpaths = [
    "tests",
]
filterwarnings = [
    "error",
    "ignore::UserWarning",
]

[dependencies]
python = "^3.8"
requests = "^2.25.0"
pandas = "^1.3.0"
numpy = "^1.21.0"

[dev-dependencies]
black = "^21.0.0"
isort = "^5.8.0"
pytest = "^6.2.0"
mypy = "^0.812"
--- END OF FILE ---

--- START OF FILE ./schema.sql ---

-- Sample database schema for the sample project
-- This demonstrates SQL file inclusion in snapshots

-- Create database
CREATE DATABASE IF NOT EXISTS sample_project;
USE sample_project;

-- Users table
CREATE TABLE IF NOT EXISTS users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    role ENUM('user', 'admin', 'moderator') DEFAULT 'user',
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    INDEX idx_username (username),
    INDEX idx_email (email),
    INDEX idx_created_at (created_at)
);

-- User profiles table
CREATE TABLE IF NOT EXISTS user_profiles (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    date_of_birth DATE,
    bio TEXT,
    avatar_url VARCHAR(500),
    timezone VARCHAR(50) DEFAULT 'UTC',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    INDEX idx_user_id (user_id)
);

-- Sessions table for authentication
CREATE TABLE IF NOT EXISTS sessions (
    id VARCHAR(255) PRIMARY KEY,
    user_id INT NOT NULL,
    ip_address VARCHAR(45),
    user_agent TEXT,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    INDEX idx_user_id (user_id),
    INDEX idx_expires_at (expires_at)
);

-- API logs table
CREATE TABLE IF NOT EXISTS api_logs (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT,
    endpoint VARCHAR(500) NOT NULL,
    method VARCHAR(10) NOT NULL,
    status_code INT NOT NULL,
    response_time_ms INT,
    ip_address VARCHAR(45),
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE SET NULL,
    INDEX idx_user_id (user_id),
    INDEX idx_endpoint (endpoint),
    INDEX idx_created_at (created_at),
    INDEX idx_status_code (status_code)
);

-- Data processing jobs table
CREATE TABLE IF NOT EXISTS processing_jobs (
    id INT AUTO_INCREMENT PRIMARY KEY,
    job_type VARCHAR(50) NOT NULL,
    status ENUM('pending', 'processing', 'completed', 'failed') DEFAULT 'pending',
    priority INT DEFAULT 0,
    data JSON,
    result JSON,
    error_message TEXT,
    started_at TIMESTAMP NULL,
    completed_at TIMESTAMP NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_status (status),
    INDEX idx_job_type (job_type),
    INDEX idx_priority (priority),
    INDEX idx_created_at (created_at)
);

-- Insert sample data
INSERT INTO users (username, email, password_hash, role) VALUES
('alice', 'alice@example.com', '$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj6fMhM6YK', 'admin'),
('bob', 'bob@example.com', '$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj6fMhM6YK', 'user'),
('charlie', 'charlie@example.com', '$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewdBPj6fMhM6YK', 'user');

INSERT INTO user_profiles (user_id, first_name, last_name, bio) VALUES
(1, 'Alice', 'Johnson', 'Project administrator and data enthusiast'),
(2, 'Bob', 'Smith', 'Regular user interested in analytics'),
(3, 'Charlie', 'Brown', 'Developer and API consumer');

-- Create view for active users with profiles
CREATE OR REPLACE VIEW active_users AS
SELECT
    u.id,
    u.username,
    u.email,
    u.role,
    CONCAT(up.first_name, ' ', up.last_name) as full_name,
    up.bio,
    u.created_at
FROM users u
LEFT JOIN user_profiles up ON u.id = up.user_id
WHERE u.is_active = TRUE;

-- Create index for performance
CREATE INDEX idx_api_logs_combined ON api_logs (user_id, created_at, status_code);

-- Sample query to get user statistics
-- SELECT
--     DATE(created_at) as date,
--     COUNT(*) as new_users,
--     COUNT(CASE WHEN role = 'admin' THEN 1 END) as admin_users
-- FROM users
-- WHERE created_at >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
-- GROUP BY DATE(created_at)
-- ORDER BY date DESC;
--- END OF FILE ---

